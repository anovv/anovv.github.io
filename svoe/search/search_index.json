{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>SVOE is a low-code framework providing scalable and highly configurable pipelines for time-series data research,  streaming and batch feature engineering, predictive model training, real-time inference and backtesting.  Built on top of Ray, the framework allows to build and scale your  custom pipelines from multi-core laptop to a cluster of 1000s of nodes.</p> <p>SVOE was originally built to accommodate a typical financial data research workflow (i.e. for Quant Researchers) with  specific data models in mind (trades, quotes, order book updates, etc., hence some examples are provided in this domain),  however the framework itself is domain-agnostic and it's components can easily be generalised and used in other fields  which rely on real-time time-series based data processing and simulation (anomaly detection, sales forecasting etc.)</p> <p></p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>SVOE consists of three main components, each providing a set of tools for a typical Quant/ML engineer workflow</p> <ul> <li>Featurizer helps defining, calculating, storing and analyzing real-time/offline time-series based features</li> <li>Trainer allows training predictive models in distributed setting using popular ML libraries (XGBoost, PyTorch)</li> <li>Backtester is used to validate and test predictive models along with user defined logic (i.e. trading strategies if used in financial domain)</li> </ul> <p>You can read more in docs</p>"},{"location":"#why-use-svoe","title":"Why use SVOE?","text":"<ul> <li>Easy to use standardized and flexible data and computation models for unified batch and stream computations - seamlessly switch between real-time and historical data for feature engineering, ML training and backtesting</li> <li>Low code, modularity and configurability - define reusable components such as  <code>FeatureDefinition</code>, <code>DataSourceDefinition</code>, <code>FeaturizerConfig</code>, <code>TrainerConfig</code>, <code>BacktesterConfig</code> etc.  to easily run your experiments</li> <li>Avoid train-predict inconsistency - Featurizer uses same feature definition for real-time inference and batch training</li> <li>No need for external data infra/DWH - Featurizer Storage allows to store and catalog computed features in any object storage while keeping index in any SQL backend, provides Data Exploration API</li> <li>Ray integration - SVOE runs wherever Ray runs (everywhere!)</li> <li>MLFlow integration - store, retrieve and analyze your ML models with MLFlow API</li> <li>Cloud / Kubernetes ready - use KubeRay or native Ray on AWS to scale out your workloads in a cloud</li> <li>Easily integrates with orchestrators (Airflow, Luigi, Prefect) - SVOE provides basic Airflow Operators for each component to easily orchestrate your workflows</li> <li>Real-time inference without MLOps burden - no need to maintain model containerization pipelines, FastAPI services and model registries. Deploy with simple Python API or yaml using InferenceLoop</li> <li>Designed for high volume low granularity data - as an example, when used in financial domain, unlike existing financial ML frameworks which use only OHLCV as a base data model, SVOE's Featurizer provides flexible tools to use and customize any data source (ticks, trades, book updates, etc.) and build streaming and historical features</li> <li>Minimized number of external dependencies - SVOE is built using Ray Core primitives and has no heavyweight external dependencies (stream processor, distributed computing engines, storages, etc.) which allows for easy deployment, maintenance and minimizes costly data transfers. The only dependency is an SQL database of user's choice. And it's all Python!</li> </ul> <p>Please refer to Installation and Quick Start for more details</p>"},{"location":"architecture-overview/","title":"Architecture Overview","text":""},{"location":"backtester-overview/","title":"Backtester","text":""},{"location":"backtester-overview/#overview","title":"Overview","text":""},{"location":"backtester-overview/#key-features","title":"Key features","text":"<ul> <li>Event-driven:</li> <li>Integrated with Featurizer: Use the same config and feature definitions to automatically construct streaming version of your features and use them for real-time model inference</li> <li>Real-time model inference: Automatically plug models from Trainer into your UDF (i.e strategy if used in financial domain) to get real-time low latency inference/predictions</li> <li>Distributed:</li> </ul>"},{"location":"backtester-quick-start/","title":"Quick Start","text":""},{"location":"backtester-quick-start/#backtester-quick-start","title":"Backtester Quick Start","text":"<p>In this example, we use Backtester in the context of financial markets, hence our user-defined logic is based on a notion of trading strategy. This can be extended to any other scenario which user wants to emulate.  Once we have our best model from Trainer, we can plug it in our <code>BaseStrategy</code> derived class and run Backtester</p> <ul> <li> <p>Define config   <pre><code>featurizer_config_path: featurizer-config.yaml\ninference_config:\n  model_uri: &lt;your-best-model-uri&gt;\n  predictor_class_name: 'XGBoostPredictor'\n  num_replicas: &lt;number-of-predictor-replicas&gt;\nsimulation_class_name: 'backtester.strategy.ml_strategy.MLStrategy'\nsimulation_params:\n  buy_delta: 0\n  sell_delta: 0\nuser_defined_params:\n  portfolio_config: &lt;portfolio_config&gt;\n  tradable_instruments_params:\n    - exchange: 'BINANCE'\n      instrument_type: 'spot'\n      symbol: 'BTC-USDT'\n</code></pre>   See MLStrategy for example implementation</p> </li> <li> <p>Run Backtester</p> CLIPython API <pre><code>svoe backtester run --config-path &lt;config-path&gt; --ray-address &lt;addr&gt; --num-workers &lt;num-workers&gt;\n</code></pre> <pre><code>config = BacktesterConfig.load_config(config_path)\nbacktester = Backtester.from_config(config)\nbacktester.run_remotely(ray_address=ray_address, num_workers=num_workers)\n</code></pre> </li> </ul> <p>This will run a distributed event-driven backtest using features and models defined earlier</p> <ul> <li> <p>Get statistics with </p> Python API <pre><code>stats = backtester.get_stats()\n</code></pre> </li> </ul>"},{"location":"featurizer-configuration/","title":"Configuration","text":""},{"location":"featurizer-configuration/#configuration","title":"Configuration","text":""},{"location":"featurizer-data-ingest/","title":"Data Ingest Pipeline","text":"<p>To get input data into SVOE, Featurizer provides two main methods: Data Ingest Pipeline and Real Time Data Recording. This section will describe the former.</p> <p>Featurizer provides a scalable, configurable and extensible data ingest pipeline which takes (offline) raw user data and puts it in Featurizer storage. It takes care of such things as indexing, compaction, per-data type resource allocation for pipeline workers and many other data engineering related problems. It integrates with DataSourceDefinition class so users can easily add their own processing logic in a modular way without spinning up and maintaining data engineering infrastructure.</p>"},{"location":"featurizer-data-ingest/#usage","title":"Usage","text":"<ul> <li>CLI: <code>svoe featurizer run-data-ingest &lt;path_to_config&gt;</code></li> <li>API: <code>FeaturizerDataIngestPipelineRunner.run(path_to_config)</code></li> </ul>"},{"location":"featurizer-data-ingest/#config","title":"Config","text":"<p>TODO describe options</p> <pre><code>provider_name: cryptotick\nbatch_size: 12\nmax_executing_tasks: 10\ndata_source_files:\n  - data_source_definition: featurizer.data_definitions.common.l2_book_incremental.cryptotick.cryptotick_l2_book_incremental.CryptotickL2BookIncrementalData\n    files_and_sizes:\n      - ['limitbook_full/20230201/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n      - ['limitbook_full/20230202/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n      - ['limitbook_full/20230203/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n      - ['limitbook_full/20230204/BINANCE_SPOT_BTC_USDT.csv.gz', 252000]\n</code></pre>"},{"location":"featurizer-data-source-definitions/","title":"DataSourceDefinition and Data Ingestion pipeline","text":""},{"location":"featurizer-data-source-definitions/#datasourcedefinition-overview","title":"DataSourceDefinition overview","text":"<p>Similar to <code>FeatureDefinition</code>, <code>DataSourceDefinition</code> defines a blueprint for a data source - a sequence  of events which framework treats as an input to our feature calculation pipelines. <code>DataSourceDefinition</code> always plays a role of a terminal node (a leaf) in a <code>FeatureDefinition</code>/<code>Feature</code> tree. Unlike <code>FeatureDefinition</code>, <code>DataSourceDefinition</code>  does not have any dependencies and does not perform any feature calculations - the purpose of this class is to hold metadata about event schema, possible preprocessing steps and other info. Let's take a look at an example <code>DataSourceDefinition</code></p> <pre><code>class MyDataSourceDefinition(DataSourceDefinition):\n\n    @classmethod\n    def event_schema(cls) -&gt; EventSchema:\n        # This method defines schema of the event\n        raise NotImplemented\n\n    @classmethod\n    def preprocess_impl(cls, raw_df: DataFrame) -&gt; DataFrame:\n        # Any preprocessing logic to get a sequence of EventSchema-like\n        # records in a form of a DataFrame from raw data (raw_df)\n        raise NotImplemented\n\n    @classmethod\n    def event_emitter_type(cls) -&gt; Type[DataSourceEventEmitter]:\n        # provides DataSourceEventEmitter type associated with this data source\n        raise NotImplemented\n</code></pre> <p>Some examples of common data source definitions</p> <ul> <li> <p>CryptotickL2BookIncrementalData - incremental updates for L2 orderbook from Cryptotick data provider</p> </li> <li> <p>CryptofeedL2BookIncrementalData - incremental updates for L2 orderbook from Cryptofeed library</p> </li> <li> <p>CryptofeedTickerData - Cryptofeed ticker</p> </li> <li> <p>CryptotickTradesData - Cryptotick trades</p> </li> <li> <p>CryptofeedTradesData - Cryptofeed trades</p> </li> </ul> <p>See Featurizer Real Time Streaming for Event Emitter details</p>"},{"location":"featurizer-feature-definition/","title":"Features and Feature Definitions","text":""},{"location":"featurizer-feature-definition/#overview","title":"Overview","text":"<p>Feature is defined as an independent time based process, a set of timestamped events with identical schemas and contextual meaning, which in the context of our framework are represented either as a real-time stream of events (in case of real-time processing) or as a sequence of recorded events, stored as a (possibly distributed) dataframe (in case of historical/batch processing)</p> <p>Feature Definition is an abstraction to define a blueprint for a time-series based feature in a modular way. In this contex, you can view a separate feature as a result of applying feature-sepicfic params to feature definition, i.e. <code>Feature = FeatureDefinition + params</code></p> <p>In code, these abstractions are represented as <code>Feature</code> and <code>FeatureDefinition</code> classes. Users are not supposed to  construct <code>Feature</code> objects directly and are expected to either use existing feature definitions or to implement their own by subclassing <code>FeatureDefinition</code>.</p>"},{"location":"featurizer-feature-definition/#featuredefinition-overview","title":"FeatureDefinition overview","text":"<p><code>FeatureDefinition</code>  is a base class for all custom feature definitions. To implement a new feature definition, user must subclass it and implement key methods. Here is an example</p> <pre><code>class MyFeatureDefinitionFD(FeatureDefinition):\n\n    @classmethod\n    def event_schema(cls) -&gt; EventSchema:\n        # This method defines schema of the event\n        raise NotImplemented\n\n    @classmethod\n    def stream(\n        cls,\n        dep_upstreams: Dict['Feature', Stream],\n        feature_params: Dict\n    ) -&gt; Union[Stream, Tuple[Stream, Any]]:\n        # Contains logic to compute events for this feature based on upstream\n        # dependencies and user-provided params.\n        # Uses Streamz library for declarative stream processing API\n        raise NotImplemented\n\n    @classmethod\n    def dep_upstream_schema(\n        cls, \n        dep_schema: str = Optional[None]\n    ) -&gt; List[Union[str, Type[DataDefinition]]]:\n        # Specifies upstream dependencies for this FeatureDefinition as a list\n        # of DataDefinition's\n        raise NotImplemented\n\n    @classmethod\n    def group_dep_ranges(\n        cls,\n        feature: Feature,\n        dep_ranges: Dict[Feature, List[BlockMeta]]\n    ) -&gt; IntervalDict:\n        # Logic to group dependant input data (dep_ranges) into atomic blocks \n        # for parallel bulk processing of each group. The most basic case is \n        # identity_grouping(...): newly produced data blocks depend only on \n        # the current block (i.e. simple map operation)\n        # For more complicated example, consider feature with 1 dependency, \n        # which produces window-aggregated calculations: here, for each new \n        # data block, we need to group all the dependant blocks which fall \n        # into that window (this is implemented in windowed_grouping(...) method)\n        # There are other more complicated cases, for example time buckets of \n        # fixed lengths (for OHLCV as an example), and custom groupings based\n        # on data blocks content (see L2SnapshotFD as an example)\n        raise NotImplemented\n</code></pre> <p>As can be seen from the example above, <code>FeatureDefintion</code> class describes a tree-like structure, where the root is the current <code>FeatureDefintion</code> and the leafs are <code>DataSourceDefinition</code> classes which produce all the dependent features. Similarly, when framework builds <code>Feature</code> objects, each object is a tree-like structure, uniquely identified by it's dependencies and parameters (see <code>Feature</code> class for more info).</p> <p>For more examples please see examples section</p>"},{"location":"featurizer-feature-definition/#defining-parameters-feature_params-and-data_params","title":"Defining parameters (feature_params and data_params)","text":"<p>WIP</p>"},{"location":"featurizer-inference-loop/","title":"Real-time model inference with Inference Loop","text":"<p>Featurizer utilizes Ray Serve to help users do real-time inference and provides an <code>InferenceLoop</code> abstraction to do asynchronous inference on streaming data.</p> <p>This allows to bypass a typical MLOps approach of creating Docker containers and FastAPI services and all the burden of maintaining relevant infrastructure.</p>"},{"location":"featurizer-inference-loop/#inferenceloop","title":"InferenceLoop","text":"<p><code>InferenceLoop</code> is a class providing a mechanism for continuous model polling and latest request result storage for downstream processing.  In a nutshell it is a separate actor/process that continuously sends requests to a model and outputs last results.  When used in offline mode, users can provide <code>Clock</code> instance to synchronize time between Featurizer event processing pipelines and inference.</p> <p>Some notable benefits of this approach:</p> <ul> <li>No need to maintain model containerization pipelines, FastAPI services and model registries. Deploy with simple Python API or yaml</li> <li>Integration with MLFlow</li> <li>Asynchronous inference allows for real-time processing without blocking on model-related calculations </li> <li>Scalable inference using adjustable <code>num_replicas</code>, indicating number of inference workers</li> <li>Decouples event processing and inference</li> <li>Use of special hardware (i.e. GPUs)</li> </ul>"},{"location":"featurizer-inference-loop/#inference-configuration","title":"Inference Configuration","text":"<p>When running an <code>InferenceLoop</code> in different scenarios (real-time Featurizer pipeline or Backtester), users can configure certain options</p> <ul> <li><code>deployment_name</code> - optional Ray Serve Deployment name</li> <li><code>model_uri</code> - path to model's artifacts as stored in MLFlow artifact storage</li> <li><code>predictor_class_name</code> - Ray Serve predictor type (ex. <code>XGBoostPredictor</code>)</li> <li><code>num_replicas</code> - number of Inference Actors to be used for this loop. Requests to workers are evenly distributed (i.e. round-robin) via http-proxy actor</li> </ul> InferenceConfigYAML <pre><code>class InferenceConfig(BaseModel):\n    deployment_name: Optional[str]\n    model_uri: str\n    predictor_class_name: str\n    num_replicas: int\n</code></pre> <pre><code>...\ninference_config:\n  model_uri: &lt;your-best-model-uri&gt;\n  predictor_class_name: 'XGBoostPredictor'\n  num_replicas: &lt;number-of-predictor-replicas&gt;\n...\n</code></pre>"},{"location":"featurizer-overview/","title":"Featurizer","text":""},{"location":"featurizer-overview/#overview","title":"Overview","text":"<p>Featurizer is a framework which combines feature storage, stream processing engine and user facing SDKs and configs to define features for real-time and batch (historical) processing on time series data. It is build on top of Ray and leverages Ray's distributed memory to produce feature sets/feature-lable sets for ML training.</p>"},{"location":"featurizer-overview/#key-features","title":"Key features","text":"<ul> <li>Flexible computation model: define modular FeatureDefinition\u2019s to calculate features on historical data as well as live data streams using the same code</li> <li>High configurability: Define yaml configs to produce feature sets/feature-label sets for unsupervised/supervised learning and analysis</li> <li>Scalable execution: Built on top of Ray, Featurizer allows for horizontally scalable stream processing and feature calculations on historical data</li> <li>Scalable data storage: Unified data model and data access API for time-series data sources and user defined features allows for easy data discovery and retrieval</li> <li>Zero-copy in-memory data access: Featurizer integrates with Ray\u2019s distributed in-memory storage, allowing to use Ray\u2019s distributed ML frameworks (XGBoost, PyTorch, RLLib) for predictive modelling without moving data to the third party storage</li> </ul>"},{"location":"featurizer-overview/#how-is-it-different-from-other-stream-processors-flink-spark-streaming","title":"How is it different from other stream processors (Flink, Spark Streaming)?","text":"<p>WIP</p>"},{"location":"featurizer-quick-start/","title":"Featurizer Quick Start","text":"<p>Featurizer helps creating distributed <code>FeatureLabelSet</code> dataframes from Feature Definitions to be used for analysis, ML training and real-time streaming.</p> <p>For this example, we will consider a scenario which often occurs in financial markets simulation, however please note that the framework is not limited to financial data and can be used with whatever scenario user provides. Here is an example to construct mid-price and volatility features from  partial order book updates, 5 second lookahead label as prediction target, using 1 second granularity data</p> <ul> <li> <p>Pick existing or define your own <code>FeatureDefinition</code> (see Feature Definitions section)</p> <ul> <li> <p>Create <code>FeaturizerConfig</code></p> <ul> <li>Define start and end dates (more in Data Model)</li> <li>Pick which features to store by setting <code>to_store</code> (more in Storage)</li> <li>Define label feature by setting <code>label_feature_index</code> and <code>label_lookahead</code> (more in Labeling)</li> <li>Define features in <code>feature_configs</code>. Each feature is a result of applying <code>params:feature</code> and    <code>params:data_source</code> to <code>FeatureDefinition</code></li> </ul> <p>Example config:</p> <p><pre><code>start_date: '2023-02-01 10:00:00'\nend_date: '2023-02-01 11:00:00'\nlabel_feature_index: 0\nlabel_lookahead: '5s'\nfeatures_to_store: [0, 1]\nfeature_configs:\n  - feature_definition: price.mid_price_fd.MidPriceFD\n    name: mid_price\n    params:\n      data_source: &amp;id001\n        - exchange: BINANCE\n          instrument_type: spot\n          symbol: BTC-USDT\n      feature:\n        sampling: 1s\n  - feature_definition: volatility.volatility_stddev_fd.VolatilityStddevFD\n    params\n      data_source: *id001\n      feature:\n        sampling: 1s\n</code></pre> See MidPriceFD and VolatilityStddevFD for implementation details</p> </li> </ul> </li> <li> <p>Run Featurizer</p> CLIPython API <pre><code>svoe featurizer run &lt;path_to_config&gt; --ray-address &lt;addr&gt; --parallelism &lt;num-workers&gt;\n</code></pre> <pre><code>Featurizer.run(path=&lt;path_to_config&gt;, ray_address=&lt;addr&gt;, parallelism=&lt;num_workers&gt;)\n</code></pre> <p>Featurizer will compile a graph of tasks, execute it in a distributed manner over the cluster and store the resulted distributed dataframe (<code>FeatureLabelSet</code>) in cluster memory and optionally in persistent storage.</p> </li> <li> <p>Get sampled results. Once calculation is finished, run following command to get sampled <code>FeatureLabelSet</code> dataframe into your local laptop memory</p> CLI <pre><code>svoe featurizer get-data --every-n &lt;every_nth_row&gt;\n</code></pre> <p>The above config will result in following dataframe: <pre><code>      timestamp  receipt_timestamp  label_mid_price-mid_price  mid_price-mid_price  feature_VolatilityStddevFD_62271b09-volatility\n0     1.675234e+09       1.675234e+09                  23084.800            23084.435                                        0.000547\n1     1.675234e+09       1.675234e+09                  23083.760            23084.355                                        0.040003\n2     1.675234e+09       1.675234e+09                  23083.505            23084.635                                        0.117757\n3     1.675234e+09       1.675234e+09                  23084.610            23085.020                                        0.257091\n4     1.675234e+09       1.675234e+09                  23084.725            23084.800                                        0.242034\n...            ...                ...                        ...                  ...                                             ...\n</code></pre></p> </li> <li> <p>Visualize the results</p> CLI <pre><code>svoe featurizer plot --every-n &lt;every_nth_row&gt;\n</code></pre> </li> </ul>"},{"location":"featurizer-real-time-data-recording/","title":"Featurizer Real Time Data Recording","text":"<p>To get input data into SVOE, Featurizer provides two main methods: Data Ingest Pipeline and Real Time Data Recording. This section will describe the latter.</p> <p>When Featurizer is run in streaming mode, users can specify which features/data sources they want to store. </p>"},{"location":"featurizer-real-time-data-recording/#config-and-cli","title":"Config and CLI","text":"<p>WIP</p>"},{"location":"featurizer-real-time-data-recording/#block-writer","title":"Block Writer","text":"<p><code>BlockWriter</code> temporarily stores all real-time events in memory and periodically dumps them to storage.  It expects <code>Compactor</code> to determine event grouping logic.</p>"},{"location":"featurizer-real-time-data-recording/#compactors","title":"Compactors","text":"<p><code>Compactor</code> derived classes contain logic to split in-memory events into blocks, which will be later put in  block storage. It expects users to override <code>compaction_split_indexes</code> which defines logic of how to group in-memory events into blocks</p> <pre><code>class Compactor:\n    def __init__(self, config):\n        self.config = config\n\n    def compaction_split_indexes(self, feature: Feature, events: List[Event]) -&gt; List[int]:\n        raise NotImplementedError\n</code></pre> <p>Default is <code>MemoryBasedCompactor</code>. It groups events into blocks of the same in-memory size, provided by user.</p>"},{"location":"featurizer-storage/","title":"Featurizer Storage","text":""},{"location":"featurizer-storage/#overview","title":"Overview","text":"<p>Featurizer stores contents of features and data sources in blocks of timestamp-sorted records. For range-based queries and  other parametrized data access and exploration it keeps an index of all the blocks metadata (i.e. start and end timestamps, in-memory and  on-disk size, user-defined parameters, etc.) in a SQL database (currently supports MySQL or SQLite). Each block is represented  as a pandas DataFrame when loaded in memory or as a gzip-compressed parquet file when stored on disk or blob storage.</p>"},{"location":"featurizer-storage/#data-models","title":"Data Models","text":"<p>There are 4 main user-facing data models</p> <ul> <li>Block</li> </ul> <p>Single block of data, currently a simple pandas dataframe <pre><code>Block = pd.DataFrame\n</code></pre></p> <ul> <li>BlockRange</li> </ul> <p>Represents a range of consecutive timestamp-sorted blocks. These are treated as a single range that contains no gaps.  Blocks are considered consecutive if time difference between them is no more than user-defined delta. <pre><code>BlockRange = List[Block]\n</code></pre></p> <ul> <li> <p>BlockMeta Represents block metadata: feature/data source name, key or id, time range, size, etc. <pre><code>BlockMeta = Dict\n</code></pre></p> </li> <li> <p>BlockRangeMeta Similarly to BlockRange, represents metadata for consecutive blocks. <pre><code>BlockRangeMeta = List[BlockMeta]\n</code></pre></p> </li> </ul>"},{"location":"featurizer-storage/#sql-tables","title":"SQL Tables","text":"<p>Metadata about features/data sources and feature/data source blocks is stored in 4 tables</p> <ul> <li><code>features_metadata</code></li> <li><code>data_sources_metadata</code></li> <li><code>feature_blocks_metadata</code></li> <li><code>data_source_blocks_metadata</code></li> </ul>"},{"location":"featurizer-storage/#data-store-adapters","title":"Data Store Adapters","text":"<p>Featurizer provides <code>DataStoreAdapter</code> class to implement custom read/write operations for block storage. Users are able to extend this class with their own logic.</p> <pre><code>class DataStoreAdapter:\n\n    def load_df(self, path: str, **kwargs) -&gt; pd.DataFrame:\n        raise NotImplementedError\n\n    def store_df(self, path: str, df: pd.DataFrame, **kwargs):\n        raise NotImplementedError\n\n    def make_feature_block_path(self, item: FeatureBlockMetadata) -&gt; str:\n        raise NotImplementedError\n\n    def make_data_source_block_path(self, item: DataSourceBlockMetadata) -&gt; str:\n        raise NotImplementedError\n</code></pre> <p>Featurizer includes implementation two data store adapters:</p> <ul> <li><code>LocalDataStoreAdapter</code> Stores and reads data from local filesystem</li> <li><code>RemoteDataStoreAdapter</code> Stores and reads data from S3</li> </ul> <p>By default, <code>LocalDataStoreAdapter</code> is used</p>"},{"location":"featurizer-storage/#data-exploration-api","title":"Data Exploration API","text":"<p>TODO implement data exploration api</p>"},{"location":"featurizer-streaming/","title":"Featurizer Real Time Streaming","text":""},{"location":"featurizer-streaming/#overview","title":"Overview","text":"<p>One of the most powerful tools of Featurizer is the ability to seamlessly switch between batch and real-time data processing without changing feature calculation code.</p> <p>Featurizer provides a set of user-facing classes to build, launch and scale real-time streaming pipelines. It is built using Streamz library to declaratively define event processing logic and  Ray Actors to scale the workload across cluster/CPUs.</p>"},{"location":"featurizer-streaming/#event-emitters","title":"Event Emitters","text":"<p><code>DataSourceEventEmitter</code> is a foundation of any streaming pipeline. Users define logic to emit <code>Event</code> objects and how to register an arbitrary callback per feature. When building a pipeline from config, Featurizer automatically  registers all necessary callbacks to connect dependent Features and calculation nodes (in case of a distributed run)</p> <pre><code>class DataSourceEventEmitter:\n\n    @classmethod\n    def instance(cls) -&gt; 'DataSourceEventEmitter':\n        raise NotImplementedError\n\n    def register_callback(self, feature: Feature, callback: Callable[[Feature, Event], Optional[Any]]):\n        raise NotImplementedError\n\n    def start(self):\n        raise NotImplementedError\n\n    def stop(self):\n        raise NotImplementedError\n</code></pre> <p>By default, Featurizer uses <code>CryptofeedEventEmitter</code> which is based on a popular Cryptofeed library.</p>"},{"location":"featurizer-streaming/#featurestreamgraph","title":"FeatureStreamGraph","text":"<p><code>FeatureStreamGraph</code>  is a class providing simple way to build a streaming pipeline in a non-distributed setting (i.e 1 worker)</p> <pre><code>fsg = FeatureStreamGraph(\n    features_or_config: Union[List[Feature], FeaturizerConfig],\n    combine_outputs: bool = False,\n    combined_out_callback: Optional[Callable[[GroupedNamedDataEvent], Any]] = None\n)\n</code></pre> <p>It uses Streamz library to connect Stream objects into a graph.</p> <ul> <li> <p><code>combine_outputs</code> defines whether output Feature streams should be merged into one</p> </li> <li> <p>If <code>combine_outputs is True</code>, <code>combined_out_callback: Callable[[GroupedNamedDataEvent], Any]</code> parameter  contains user-defined logic to process newly produced combined event.  <code>GroupedNamedDataEvent</code> describes data events for all features grouped into a single object.  This is useful for real-time ML inference, where models expect all feature values in a single request</p> </li> </ul> <p>There are a number of useful methods to build custom pipelines:</p> <ul> <li> <p><code>def emit_named_data_event(self, named_event: NamedDataEvent)</code></p> <p>Emits new event into the graph</p> </li> <li> <p><code>def set_callback(self, feature: Feature, callback: Callable[[Feature, Event], Optional[Any]])</code></p> <p>Sets custom callback per feature event</p> </li> <li> <p><code>def get_ins(self) -&gt; List[Feature]</code></p> <p>Lists input feature streams</p> </li> <li> <p><code>def get_outs(self) -&gt; List[Feature]</code></p> <p>Lists output feature streams</p> </li> <li> <p><code>def get_stream(self, feature: Feature) -&gt; Stream</code></p> <p>Gets stream for feature</p> </li> </ul> <p>When initialized with a <code>FeaturizerConfig</code>, Featurizer automatically builds necessary <code>FeatureStreamGraph</code> objects and connects them with corresponding Event Emitters/ </p>"},{"location":"featurizer-streaming/#data-recording","title":"Data Recording","text":"<p>Users can configure Featurizer to store features/data sources events to Featurizer Storage for further processing. See Featurizer Real Time Data Recording.</p>"},{"location":"featurizer-streaming/#simulating-real-time-stream-from-offline-data-with-offlinefeaturestreamgenerator","title":"Simulating real-time stream from offline data with OfflineFeatureStreamGenerator","text":"<p>For backtesting/simulation/ML model validation purposes, we often need to be able to simulate a data stream from stored events. Featurizer provides OfflineFeatureStreamGenerator class, which implements a typical generator interface and can be used to run custom logic over stored data stream.</p> <p>Work In Progress (make OfflineFeatureEventEmitter)</p> <pre><code>OfflineFeatureStreamGenerator example\n</code></pre> <p>See more in <code>featurizer.feature_stream.offline_feature_stream_generator.py</code></p>"},{"location":"featurizer-streaming/#scalability","title":"Scalability","text":"<p>WIP</p> <p><code>FeatureStreamWorkerGraph</code></p> <p><code>ScalingStrategy</code></p> <p><code>FeaturizerStreamWorkerActor</code> and <code>FeaturizerStreamManagerActor</code></p> <p><code>Transport</code></p>"},{"location":"featurizer-streaming/#fault-tolerance","title":"Fault Tolerance","text":"<p>Work In Progress</p>"},{"location":"featurizer-streaming/#visualization","title":"Visualization","text":"<p>Work In Progress</p>"},{"location":"featurizer-synthetic-data-sources/","title":"Synthetic Data Sources","text":""},{"location":"featurizer-task-graph/","title":"Featurizer Task Graph","text":""},{"location":"featurizer-task-graph/#overview","title":"Overview","text":"<p>The core of Featurizer's offline feature calculation is in building and executing Task Graph.</p> <p>Task Graph is a graph made of load, preprocess and user-defined feature specific calculation logic tasks automatically built by the framework. See more in <code>featurizer.task_graph.builder.py</code></p>"},{"location":"featurizer-task-graph/#how-building-featureset-graph-works","title":"How Building FeatureSet Graph Works","text":"<p>As mentioned in Features and Feature Definitions section, each <code>Feature</code> is a tree-like structure with <code>DataSource</code> leafs, dependent features as nodes and target feature as a root. When user defines all necessary features in a <code>FeaturizerConfig</code>, the framework builds all the feature trees and their interdependencies and then maps corresponding feature calculation tasks (which are defined by <code>FeatureDefinition:stream</code> method) and data source load tasks into a graph. The tasks are interconnected by passing dataframes as inputs and returns, which are stored in Ray's distributed memory</p> <p>When user specifies time range, Featurizer fetches metadata about all dependent data source blocks and builds final graph by applying tree-like features to each block (and possible others, which are defined by user in  <code>FeatureDefinition:group_dep_ranges</code>)</p>"},{"location":"featurizer-task-graph/#labels-and-featurelabelset-graph","title":"Labels and FeatureLabelSet Graph","text":"<p>When building data sets for supervised learning, users need to defined labels to be used as target values. Users can pick any feature in a feature set as a label by setting specific config option. Since we operate on time series data and solve time-based prediction problem, this feature/label will be shifted ahead  relatively to all other features by a time period specified by user, this way we will produce FeatureLabelSet ready to be used as an input for predictive ML models training/validation/testing steps.</p> <p>Config options:</p> <ul> <li> <p>name or index of a feature to mark as a label <pre><code>label_feature: 0\n</code></pre></p> </li> <li> <p>how far ahead we want to shift labeled timeseries to be used as predictor targets <pre><code>label_lookahead: '10s'\n</code></pre></p> </li> </ul>"},{"location":"featurizer-task-graph/#point-in-time-joins","title":"Point-in-time Joins","text":"<p>Since features may have different time granularity and/or irregular frequencies, in order to produce unified  dataframe with multiple features we need to perform a join operation. The framework automatically builds join tasks and takes care of updating resulting graph to produce unified dataframe (see picture).</p> <p></p>"},{"location":"featurizer-task-graph/#execution","title":"Execution","text":"<p>After Task Graph is build, the framework submits it to Ray cluster for execution. Sub-graphs are submitted in batches defined by <code>parallelism</code> parameter:</p> <pre><code>Featurizer.run(config=&lt;config&gt;, ray_address=&lt;ray_address&gt;, parallelism=&lt;parallelism&gt;)\n</code></pre> <p>Ray automatically takes care of executing them amongst workers. In order to effectively  see speed-up from horizontal scaling users need to make sure that the cluster has more cores than <code>parallelism</code>.</p> <p>Oncec execution is finished, the resulting dataframes are stored in cluster's distributed memory and can be accessed via <code>Featurizer.get_dataset()</code></p> <p>More in <code>featurizer.task_graph.executor.py</code></p>"},{"location":"featurizer-transforms/","title":"Transforms","text":""},{"location":"featurizer-transforms/#feature-transforms","title":"Feature Transforms","text":"<p>Featurizer provides a set of built-in operators for common transformations on time-series data called Transforms</p> <p>Some examples are:</p> <ul> <li>diff - transforms time-series into a series of differences between sequential events</li> <li>min_max_scaling - scales values between 0 and 1 based on min and max value in a window</li> <li>ewma - exponential smoothing</li> </ul> <p>Transforms are similar to regular FeatureDefinitions when coming to definition (they subclass <code>FeatureDefinition</code>), however differ in how they are used in <code>FeaturizerConfig</code>: unlike regular <code>FeatureDefinition</code> which do not need direct specification of dependant features (they are constructed automatically), Transforms require specific definition of dependencies:</p> <pre><code>feature_configs:\n  - feature_definition: transforms.diff\n    name: diff_mid_price\n    deps:\n      - mid_price\n    params:\n        ...\n  - feature_definition: price.mid_price\n    name: mid_price\n    params:\n        ...\n</code></pre>"},{"location":"featurizer-transforms/#creating-new-transforms","title":"Creating new transforms","text":"<p>WIP</p>"},{"location":"installation/","title":"Installation","text":"<p>Install from PyPi. Be aware that Svoe requires Python 3.10+.</p> <pre><code>pip install svoe\n</code></pre> <p>For local environment launch standalone setup on your laptop. This will start local Ray cluster, create and populate  SQLite database, spin up MLFlow tracking server and load sample data from remote store (S3). Make sure you have all necessary dependencies present <pre><code>svoe standalone\n</code></pre></p> <p>For distributed setting, please refer to Running on remote clusters</p>"},{"location":"quick-start/","title":"Quick start","text":"<p>For this example, we will consider a scenario which often occurs in financial markets simulation, however please note that the framework is not limited to financial data and can be used with whatever scenario user provides. As an example, here is a simple 3 step tutorial to build a simple mid-price prediction model based on past price and volatility. </p> <ul> <li> <p>Run Featurizer to construct mid-price and volatility features from partial order book updates, 5 second lookahead label as prediction target, using 1 second granularity data </p> <ul> <li>Define <code>featurizer-config.yaml</code> <pre><code>start_date: '2023-02-01 10:00:00'\nend_date: '2023-02-01 11:00:00'\nlabel_feature_index: 0\nlabel_lookahead: '5s'\nfeatures_to_store: [0, 1]\nfeature_configs:\n  - feature_definition: price.mid_price_fd.MidPriceFD\n    name: mid_price\n    params:\n      data_source: &amp;id001\n        - exchange: BINANCE\n          instrument_type: spot\n          symbol: BTC-USDT\n      feature:\n        sampling: 1s\n  - feature_definition: volatility.volatility_stddev_fd.VolatilityStddevFD\n    params\n      data_source: *id001\n      feature:\n        sampling: 1s\n</code></pre>   See MidPriceFD and VolatilityStddevFD for implementation details</li> <li> <p>Run Featurizer</p> CLIPython API <pre><code>svoe featurizer run &lt;path_to_config&gt; --ray-address &lt;addr&gt; --parallelism &lt;num-workers&gt;\n</code></pre> <pre><code>Featurizer.run(path=&lt;path_to_config&gt;, ray_address=&lt;addr&gt;, parallelism=&lt;num_workers&gt;)\n</code></pre> </li> <li> <p>Once calculation is finished, load sampled <code>FeatureLabelSet</code> dataframe to your local client</p> CLI <pre><code>svoe featurizer get-data --every-n &lt;every_nth_row&gt;\n</code></pre> <p>This produces       <pre><code>      timestamp  receipt_timestamp  label_mid_price-mid_price  mid_price-mid_price  feature_VolatilityStddevFD_62271b09-volatility\n0     1.675234e+09       1.675234e+09                  23084.800            23084.435                                        0.000547\n1     1.675234e+09       1.675234e+09                  23083.760            23084.355                                        0.040003\n2     1.675234e+09       1.675234e+09                  23083.505            23084.635                                        0.117757\n3     1.675234e+09       1.675234e+09                  23084.610            23085.020                                        0.257091\n4     1.675234e+09       1.675234e+09                  23084.725            23084.800                                        0.242034\n...            ...                ...                        ...                  ...                                             ...\n</code></pre></p> </li> <li> <p>We can also visualize the results</p> CLI <pre><code>svoe featurizer plot --every-n &lt;every_nth_row&gt;\n</code></pre> </li> </ul> </li> <li> <p>Once we have our <code>FeatureLabelSet</code> calculated and loaded in cluster memory, let's use Trainer to train XGBoost model to predict mid-price 5 seconds ahead, validate the model, tune hyperparams and pick best model</p> <ul> <li>Define config     <pre><code>xgboost:\n  params:\n    tree_method: 'approx'\n    objective: 'reg:linear'\n    eval_metric: [ 'logloss', 'error' ]\n  num_boost_rounds: 10\n  train_valid_test_split: [0.5, 0.3]\nnum_workers: 3\ntuner_config:\n  param_space:\n    params:\n      max_depth:\n        randint:\n          lower: 2\n          upper: 8\n      min_child_weight:\n        randint:\n          lower: 1\n          upper: 10\n  num_samples: 8\n  metric: 'train-logloss'\n  mode: 'min'\nmax_concurrent_trials: 3\n</code></pre></li> <li> <p>Run Trainer</p> CLIPython API <pre><code>svoe trainer run --config-path &lt;config-path&gt; --ray-address &lt;addr&gt;\n</code></pre> <pre><code>config = TrainerConfig.load_config(config_path)\ntrainer_manager = TrainerManager(config=config, ray_address=ray_address)\ntrainer_manager.run(trainer_run_id='sample-run-id', tags={})\n</code></pre> </li> <li> <p>Visualize predictions</p> CLI <pre><code>svoe trainer predictions --model-uri &lt;model-uri&gt;\n</code></pre> </li> <li> <p>Select best model</p> CLIPython API <pre><code>svoe trainer best-model --metric-name valid-logloss --mode min\n</code></pre> <pre><code>best-model-uri = mlflow_client.get_best_checkpoint_uri(metric_name=metric_name, experiment_name=experiment_name, mode=mode)\n</code></pre> </li> </ul> </li> <li> <p>In this example, we use Backtester in the context of financial markets, hence our user-defined logic is based on a notion of trading strategy. This can be extended to any other scenario which user wants to emulate. Once we have our best model, we can plug it in our <code>BaseStrategy</code> derived class and run Backtester</p> <ul> <li> <p>Define config   <pre><code>featurizer_config_path: featurizer-config.yaml\ninference_config:\n  model_uri: &lt;your-best-model-uri&gt;\n  predictor_class_name: 'XGBoostPredictor'\n  num_replicas: &lt;number-of-predictor-replicas&gt; \nsimulation_class_name: 'backtester.strategy.ml_strategy.MLStrategy'\nsimulation_params:\n  buy_delta: 0\n  sell_delta: 0\nuser_defined_params:\n  portfolio_config: &lt;portfolio_config&gt;\n  tradable_instruments_params:\n    - exchange: 'BINANCE'\n      instrument_type: 'spot'\n      symbol: 'BTC-USDT'\n</code></pre> See MLStrategy for example implementation</p> </li> <li> <p>Run Backtester</p> CLIPython API <pre><code>svoe backtester run --config-path &lt;config-path&gt; --ray-address &lt;addr&gt; --num-workers &lt;num-workers&gt;\n</code></pre> <pre><code>config = BacktesterConfig.load_config(config_path)\nbacktester = Backtester.from_config(config)\nbacktester.run_remotely(ray_address=ray_address, num_workers=num_workers)\n</code></pre> </li> </ul> <p>This will run a distributed event-driven backtest using features and models defined earlier</p> <ul> <li> <p>Get statistics with </p> Python API <pre><code>stats = backtester.get_stats()\n</code></pre> </li> </ul> </li> </ul>"},{"location":"trainer-overview/","title":"Trainer","text":""},{"location":"trainer-overview/#overview","title":"Overview","text":""},{"location":"trainer-overview/#key-features","title":"Key features","text":"<ul> <li>Highly configurable: Trainer provides unified API and configs to train and evaluate predictive models using various ML libraries (XGBoost, PyTorch, RLLib)</li> <li>Integrated with Featurizer: Use distributed in-memory FeatureLabelSet data structure to train your predictive models without extra data pipelines</li> <li>Scalability: Data-parallel distributed training for all supported frameworks</li> <li>Hyperparameter optimization: Use Ray Tune to optimize hyperparameters and pick the best model</li> <li>Model and metadata storage: Trainer provides easy API for model access and metadata discovery by integrating with MLFlow</li> </ul>"},{"location":"trainer-quick-start/","title":"Quick Start","text":""},{"location":"trainer-quick-start/#trainer-quick-start","title":"Trainer Quick Start","text":"<p>Once we have our <code>FeatureLabelSet</code> calculated and loaded in cluster memory using Featurizer, let's use Trainer to train XGBoost model to predict mid-price 5 seconds ahead, validate the model, tune hyperparams and pick best model</p> <ul> <li>Define config     <pre><code>xgboost:\n  params:\n    tree_method: 'approx'\n    objective: 'reg:linear'\n    eval_metric: [ 'logloss', 'error' ]\n  num_boost_rounds: 10\n  train_valid_test_split: [0.5, 0.3]\nnum_workers: 3\ntuner_config:\n  param_space:\n    params:\n      max_depth:\n        randint:\n          lower: 2\n          upper: 8\n      min_child_weight:\n        randint:\n          lower: 1\n          upper: 10\n  num_samples: 8\n  metric: 'train-logloss'\n  mode: 'min'\nmax_concurrent_trials: 3\n</code></pre></li> <li> <p>Run Trainer</p> CLIPython API <pre><code>svoe trainer run --config-path &lt;config-path&gt; --ray-address &lt;addr&gt;\n</code></pre> <pre><code>config = TrainerConfig.load_config(config_path)\ntrainer_manager = TrainerManager(config=config, ray_address=ray_address)\ntrainer_manager.run(trainer_run_id='sample-run-id', tags={})\n</code></pre> </li> <li> <p>Visualize predictions</p> CLI <pre><code>svoe trainer predictions --model-uri &lt;model-uri&gt;\n</code></pre> </li> <li> <p>Select best model</p> CLIPython API <pre><code>svoe trainer best-model --metric-name valid-logloss --mode min\n</code></pre> <pre><code>best-model-uri = mlflow_client.get_best_checkpoint_uri(metric_name=metric_name, experiment_name=experiment_name, mode=mode)\n</code></pre> </li> </ul>"}]}